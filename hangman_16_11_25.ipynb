{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4341e315-cbbf-4a33-b119-bf83827eb641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ortalrex\\AppData\\Local\\Temp\\ipykernel_31928\\1316263417.py:370: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.my_bert_model.load_state_dict(torch.load(\"hangman_model_12_09.pt\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "=== Single Random Word Test ===\n",
      "Starting game with word: _ _ _ _ _ _ _ _ _ _ _\n",
      "Guess: e -> _ e _ _ _ _ _ _ _ _ _ | Incorrect: 0/5\n",
      "Guess: i -> _ e _ _ _ _ _ _ _ _ _ | Incorrect: 1/5\n",
      "Guess: a -> _ e _ _ _ _ a _ _ _ _ | Incorrect: 1/5\n",
      "Guess: t -> _ e _ _ _ _ a t _ _ _ | Incorrect: 1/5\n",
      "Guess: n -> _ e _ _ _ _ a t _ _ _ | Incorrect: 2/5\n",
      "Guess: r -> r e _ _ r _ a t _ r _ | Incorrect: 2/5\n",
      "Guess: o -> r e _ o r _ a t o r _ | Incorrect: 2/5\n",
      "Guess: m -> r e _ o r m a t o r _ | Incorrect: 2/5\n",
      "Guess: f -> r e f o r m a t o r _ | Incorrect: 2/5\n",
      "Guess: y -> r e f o r m a t o r y | Incorrect: 2/5\n",
      "SUCCESS! Word guessed: reformatory\n",
      "\n",
      "=== Multiple Words Test ===\n",
      "\n",
      "=== Game 1: Testing word 'deathdeep' ===\n",
      "Starting game with word: _ _ _ _ _ _ _ _ _\n",
      "Guess: e -> _ e _ _ _ _ e e _ | Incorrect: 0/5\n",
      "Guess: r -> _ e _ _ _ _ e e _ | Incorrect: 1/5\n",
      "Guess: s -> _ e _ _ _ _ e e _ | Incorrect: 2/5\n",
      "Guess: t -> _ e _ t _ _ e e _ | Incorrect: 2/5\n",
      "Guess: n -> _ e _ t _ _ e e _ | Incorrect: 3/5\n",
      "Guess: w -> _ e _ t _ _ e e _ | Incorrect: 4/5\n",
      "Guess: p -> _ e _ t _ _ e e p | Incorrect: 4/5\n",
      "Guess: k -> _ e _ t _ _ e e p | Incorrect: 5/5\n",
      "Guess: a -> _ e a t _ _ e e p | Incorrect: 5/5\n",
      "Guess: h -> _ e a t h _ e e p | Incorrect: 5/5\n",
      "Guess: d -> d e a t h d e e p | Incorrect: 5/5\n",
      "SUCCESS! Word guessed: deathdeep\n",
      "\n",
      "=== Game 2: Testing word 'sillographer' ===\n",
      "Starting game with word: _ _ _ _ _ _ _ _ _ _ _ _\n",
      "Guess: e -> _ _ _ _ _ _ _ _ _ _ e _ | Incorrect: 0/5\n",
      "Guess: d -> _ _ _ _ _ _ _ _ _ _ e _ | Incorrect: 1/5\n",
      "Guess: r -> _ _ _ _ _ _ r _ _ _ e r | Incorrect: 1/5\n",
      "Guess: a -> _ _ _ _ _ _ r a _ _ e r | Incorrect: 1/5\n",
      "Guess: t -> _ _ _ _ _ _ r a _ _ e r | Incorrect: 2/5\n",
      "Guess: i -> _ i _ _ _ _ r a _ _ e r | Incorrect: 2/5\n",
      "Guess: s -> s i _ _ _ _ r a _ _ e r | Incorrect: 2/5\n",
      "Guess: g -> s i _ _ _ g r a _ _ e r | Incorrect: 2/5\n",
      "Guess: p -> s i _ _ _ g r a p _ e r | Incorrect: 2/5\n",
      "Guess: h -> s i _ _ _ g r a p h e r | Incorrect: 2/5\n",
      "Guess: o -> s i _ _ o g r a p h e r | Incorrect: 2/5\n",
      "Guess: c -> s i _ _ o g r a p h e r | Incorrect: 3/5\n",
      "Guess: n -> s i _ _ o g r a p h e r | Incorrect: 4/5\n",
      "Guess: l -> s i l l o g r a p h e r | Incorrect: 4/5\n",
      "SUCCESS! Word guessed: sillographer\n",
      "\n",
      "=== Game 3: Testing word 'stellification' ===\n",
      "Starting game with word: _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "Guess: i -> _ _ _ _ _ i _ i _ _ _ i _ _ | Incorrect: 0/5\n",
      "Guess: t -> _ t _ _ _ i _ i _ _ t i _ _ | Incorrect: 0/5\n",
      "Guess: a -> _ t _ _ _ i _ i _ a t i _ _ | Incorrect: 0/5\n",
      "Guess: n -> _ t _ _ _ i _ i _ a t i _ n | Incorrect: 0/5\n",
      "Guess: o -> _ t _ _ _ i _ i _ a t i o n | Incorrect: 0/5\n",
      "Guess: s -> s t _ _ _ i _ i _ a t i o n | Incorrect: 0/5\n",
      "Guess: f -> s t _ _ _ i f i _ a t i o n | Incorrect: 0/5\n",
      "Guess: c -> s t _ _ _ i f i c a t i o n | Incorrect: 0/5\n",
      "Guess: r -> s t _ _ _ i f i c a t i o n | Incorrect: 1/5\n",
      "Guess: l -> s t _ l l i f i c a t i o n | Incorrect: 1/5\n",
      "Guess: e -> s t e l l i f i c a t i o n | Incorrect: 1/5\n",
      "SUCCESS! Word guessed: stellification\n",
      "\n",
      "=== Game 4: Testing word 'interiorizing' ===\n",
      "Starting game with word: _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "Guess: i -> i _ _ _ _ i _ _ i _ i _ _ | Incorrect: 0/5\n",
      "Guess: n -> i n _ _ _ i _ _ i _ i n _ | Incorrect: 0/5\n",
      "Guess: g -> i n _ _ _ i _ _ i _ i n g | Incorrect: 0/5\n",
      "Guess: a -> i n _ _ _ i _ _ i _ i n g | Incorrect: 1/5\n",
      "Guess: s -> i n _ _ _ i _ _ i _ i n g | Incorrect: 2/5\n",
      "Guess: l -> i n _ _ _ i _ _ i _ i n g | Incorrect: 3/5\n",
      "Guess: z -> i n _ _ _ i _ _ i z i n g | Incorrect: 3/5\n",
      "Guess: o -> i n _ _ _ i o _ i z i n g | Incorrect: 3/5\n",
      "Guess: t -> i n t _ _ i o _ i z i n g | Incorrect: 3/5\n",
      "Guess: r -> i n t _ r i o r i z i n g | Incorrect: 3/5\n",
      "Guess: e -> i n t e r i o r i z i n g | Incorrect: 3/5\n",
      "SUCCESS! Word guessed: interiorizing\n",
      "\n",
      "=== Game 5: Testing word 'remnants' ===\n",
      "Starting game with word: _ _ _ _ _ _ _ _\n",
      "Guess: e -> _ e _ _ _ _ _ _ | Incorrect: 0/5\n",
      "Guess: r -> r e _ _ _ _ _ _ | Incorrect: 0/5\n",
      "Guess: s -> r e _ _ _ _ _ s | Incorrect: 0/5\n",
      "Guess: t -> r e _ _ _ _ t s | Incorrect: 0/5\n",
      "Guess: c -> r e _ _ _ _ t s | Incorrect: 1/5\n",
      "Guess: i -> r e _ _ _ _ t s | Incorrect: 2/5\n",
      "Guess: a -> r e _ _ a _ t s | Incorrect: 2/5\n",
      "Guess: n -> r e _ n a n t s | Incorrect: 2/5\n",
      "Guess: g -> r e _ n a n t s | Incorrect: 3/5\n",
      "Guess: u -> r e _ n a n t s | Incorrect: 4/5\n",
      "Guess: m -> r e m n a n t s | Incorrect: 4/5\n",
      "SUCCESS! Word guessed: remnants\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Successful games: 5/5\n",
      "Failed games: 0/5\n",
      "Average guesses per successful game: 11.60\n",
      "Accuracy of 1.0 in 5 games\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import copy\n",
    "import itertools\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "\n",
    "\n",
    "model_allready_trained = True    # change to True if you want to load the model I send \"hangman_model_12_09.pt\" and not start with new training.\n",
    "run_in_googel_colab = False       # change to True if you want to run on googel colab\n",
    "\n",
    "\n",
    "if run_in_googel_colab == True:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# and I added this two classes: MyBertModelForHangman and ApplyBert\n",
    "class MyBertModelForHangman(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim_n = None, heads_n = None, layers_n = None, max_word_length = None, dropouts_rate = None):\n",
    "        super().__init__()\n",
    "        self.model_dim_n = model_dim_n\n",
    "        self.heads_n = heads_n\n",
    "        self.layers_n = layers_n\n",
    "        self.max_word_length = max_word_length\n",
    "        self.dropouts_rate = dropouts_rate\n",
    "\n",
    "        #create dictionary that convert letter to numbers\n",
    "        alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.dict_for_letter_to_number = {'[PAD]': 0, '_': 1, '[UNK]': 2}\n",
    "        self.dict_for_letter_to_number.update({letter: i + 3 for i, letter in enumerate(alphabet)})\n",
    "\n",
    "        # create instances\n",
    "        self.embedding = nn.Embedding(len(self.dict_for_letter_to_number), self.model_dim_n, padding_idx = 0)\n",
    "        # create positional encoding\n",
    "        self.create_positional_encoding()\n",
    "        self.create_transformer_layers()\n",
    "        self.apply_dropout = nn.Dropout(self.dropouts_rate)\n",
    "        self.forward_projection = nn.Linear(self.model_dim_n, len(self.dict_for_letter_to_number))\n",
    "\n",
    "\n",
    "    def tokenize_word_from_dictionary(self, word):\n",
    "\n",
    "        tokenized_word = [self.dict_for_letter_to_number.get(letter, self.dict_for_letter_to_number['[UNK]']) for letter in word]     # in each word convert the letter to number according to dict_for_letter_to_number. If there is unknow character put there the \"unknow\" token.\n",
    "\n",
    "        while len(tokenized_word) < self.max_word_length:                                                                             # pad the word with the pad token until it reach the max lenth\n",
    "            tokenized_word.append(self.dict_for_letter_to_number['[PAD]'])\n",
    "\n",
    "        return tokenized_word\n",
    "\n",
    "\n",
    "    def tokenize_mask_word(self, masked_word):\n",
    "        '''Return a tuple of 2 list: list of the tokeniezed word and list of position of the mask letters.'''\n",
    "        masked_word_without_spaces = masked_word[::2]  # Remove spaces\n",
    "\n",
    "        tokenized_masked_word = [self.dict_for_letter_to_number.get(letter, self.dict_for_letter_to_number['[UNK]']) for letter in masked_word_without_spaces]  # as in the tokenize_word_from_dictionary function.\n",
    "\n",
    "        while len(tokenized_masked_word) < self.max_word_length:                                                                                                # as in the tokenize_word_from_dictionary function.\n",
    "            tokenized_masked_word.append(self.dict_for_letter_to_number['[PAD]'])\n",
    "\n",
    "        ind_of_masked_letter = [index for index, token in enumerate(tokenized_masked_word) if token == self.dict_for_letter_to_number['_']]                # find position of masked letters\n",
    "\n",
    "        return tokenized_masked_word, ind_of_masked_letter\n",
    "\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        positional_encoding_matrix = torch.zeros(self.max_word_length, self.model_dim_n)         # create a tensor of 0's with the propoer dimentions\n",
    "\n",
    "        letter_position = torch.arange(0, self.max_word_length).unsqueeze(1).float()                          # create a tensor with the proper structure for letter, for exampl: [[0.], [1.], [2.], [3.], [4.]]\n",
    "\n",
    "        the_divided_term = torch.exp(torch.arange(0, self.model_dim_n,2).float() * - (math.log(10000) / self.model_dim_n))      # it term to divide the position after applyn to it cos or sin based on the position, based on the formula in attention is all you need.\n",
    "        positional_encoding_matrix[:, 0::2] = torch.sin(letter_position * the_divided_term)                                     # for all even values apply this formula\n",
    "        positional_encoding_matrix[:, 1::2] = torch.cos(letter_position * the_divided_term)                                     # for all not even this\n",
    "\n",
    "        self.register_buffer('positional_encoding', positional_encoding_matrix.unsqueeze(0))                                                             # keep the positional encoding fixed and not train it\n",
    "\n",
    "\n",
    "    def create_transformer_layers(self):\n",
    "\n",
    "        self.attention_layer = nn.ModuleList([nn.MultiheadAttention(embed_dim = self.model_dim_n, num_heads = self.heads_n, dropout = self.dropouts_rate, batch_first=True) for _ in range(self.layers_n)])   #  self-attention layers: Each layer helps the model \"pay attention\" to all tokens in the input sequence\n",
    "\n",
    "        self.norm_layer_phase_1 = nn.ModuleList([nn.LayerNorm(self.model_dim_n) for _ in range(self.layers_n)])     #  after attention, normalize the activations for stable training\n",
    "\n",
    "        self.norm_layer_phase_2 = nn.ModuleList([nn.LayerNorm(self.model_dim_n) for _ in range(self.layers_n)])     # second layer normalization: After the feedforward step, normalize again\n",
    "\n",
    "        self.feed_forward_layer = nn.ModuleList([                                                                 # neural network for deeper learning\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.model_dim_n, self.model_dim_n * 4),  # First, expand the hidden size\n",
    "                nn.GELU(),                                  # Use GELU activation for non-linearity (used in BERT)\n",
    "                nn.Dropout(self.dropouts_rate),\n",
    "                nn.Linear(self.model_dim_n * 4, self.model_dim_n)) for _ in range(self.layers_n)])   # Then project back down to original size\n",
    "\n",
    "\n",
    "    def apply_the_network(self, x, padding_mask = None): # apply the bert nn architecture, the input x is tokenzed tensor\n",
    "        x = self.embedding(x) * math.sqrt(self.model_dim_n)  # create embedding and scale by model dimentions\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :] \n",
    "        x = self.apply_dropout(x)\n",
    "\n",
    "        for layer_index in range(self.layers_n):        # apply transformer block\n",
    "            attention_results, _ = self.attention_layer[layer_index](x, x, x, key_padding_mask = padding_mask) \n",
    "            x = self.norm_layer_phase_1[layer_index](x + self.apply_dropout(attention_results))\n",
    "            feed_forward_results = self.feed_forward_layer[layer_index](x)\n",
    "            x = self.norm_layer_phase_2[layer_index](x + self.apply_dropout(feed_forward_results))\n",
    "\n",
    "        x = self.forward_projection(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "## I finished the model definition, now creating class of function that apply the model:\n",
    "class ApplyBert():\n",
    "\n",
    "    def __init__(self, train_dataset_path):\n",
    "        self.my_bert_model = None    # will be update in the hyper parameter search function that also run the full training\n",
    "        self.train_dataset_path = train_dataset_path\n",
    "        self.create_train_validation_words()\n",
    "        self.running_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def create_train_validation_words(self):\n",
    "\n",
    "        with open(self.train_dataset_path, 'r') as file:\n",
    "            train_words = [line.strip().lower() for line in file] # word is list of words\n",
    "\n",
    "        self.train_words, self.validation_words = train_test_split(train_words, test_size = 0.2)   # split for 80% train, 20% validation\n",
    "\n",
    "\n",
    "    def convert_batch_words_to_masked_words(self, batch_words, max_mask_letter_rate, versions_per_word = 2):\n",
    "\n",
    "        # create masked inpute\n",
    "        batch_mask_inputs = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for word in batch_words:\n",
    "            tokenized_word = self.my_bert_model.tokenize_word_from_dictionary(word)[: self.my_bert_model.max_word_length]  # include padding\n",
    "\n",
    "            # Create 2 masked versions of the same word\n",
    "            for _ in range(versions_per_word):\n",
    "                # Different random masking each time\n",
    "                inds_of_masked_positions = random.sample(range(len(word)),                                                   # from all the indexes of the word\n",
    "                                                        max(1, int(len(word) * random.uniform(0.2, max_mask_letter_rate))))  # chose the max from 1 or distrubute uniformly between this 2 values.\n",
    "\n",
    "                masked_tokenized_word = tokenized_word.copy()\n",
    "                tokenized_word_label_letters = [-100] * len(tokenized_word)      # give me list in the lenth of the word - max length\n",
    "\n",
    "                for position in inds_of_masked_positions:\n",
    "                    masked_tokenized_word[position] = self.my_bert_model.dict_for_letter_to_number['_']\n",
    "                    tokenized_word_label_letters[position] = tokenized_word[position]\n",
    "\n",
    "                batch_mask_inputs.append(masked_tokenized_word)\n",
    "                batch_labels.append(tokenized_word_label_letters)\n",
    "\n",
    "        batch_mask_inputs = torch.tensor(batch_mask_inputs, dtype = torch.long).to(self.running_device)      # tokenies masked words: 4,7,_,6,9,0,0,0 ([PAD],[PAD],[PAD])\n",
    "        batch_labels = torch.tensor(batch_labels, dtype = torch.long).to(self.running_device)                # the true label of the masked words, the not maske are -100 and ignored -100,-100,8,-100,-100,-100,-100,-100\n",
    "\n",
    "        return batch_mask_inputs, batch_labels\n",
    "\n",
    "\n",
    "\n",
    "    def bert_training(self, epochs_n, train_words, validation_words, batch_size, learning_rate, after_what_number_of_failed_epoch_to_stop, max_mask_letter_rate):\n",
    "\n",
    "        print(f'Apply train with {len(train_words)} words and validate on {len(validation_words)} words')\n",
    "\n",
    "        # variables to early stopping in the end of this function\n",
    "        patience_counter = 0\n",
    "        best_model_so_far = None\n",
    "        best_validation_loss = float('inf')\n",
    "\n",
    "        ###### training\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.my_bert_model.parameters(), lr = learning_rate)     # define how to minimize the loss. In this case same as adam, because there is no regularization.\n",
    "        loss_measurement = nn.CrossEntropyLoss(ignore_index = -100)                         # mesure the goodness of the prediction, ignore not masked letters. it is the average of the âˆ’log(predictedÂ probabilityÂ ofÂ theÂ correctÂ class), so as the prediction increaes the loss decreased.\n",
    "\n",
    "\n",
    "        for epoch in range(epochs_n):             # for each training circle\n",
    "\n",
    "            # Initialize variables\n",
    "            epoch_start_time = time.time()\n",
    "            train_total_loss = 0\n",
    "            train_batches_n = 0\n",
    "\n",
    "            self.my_bert_model.train()\n",
    "\n",
    "            random.shuffle(train_words)\n",
    "\n",
    "            for index in range(0, len(train_words), batch_size):   # for each batch\n",
    "\n",
    "                batch_words = train_words[index : index + batch_size]\n",
    "\n",
    "                batch_mask_inputs, batch_labels = self.convert_batch_words_to_masked_words(batch_words, max_mask_letter_rate)\n",
    "\n",
    "                # move the batch forward to see prediction\n",
    "                logits = self.my_bert_model.apply_the_network(batch_mask_inputs, padding_mask = (batch_mask_inputs == 0))     # do True where pad and after that ignore in the attention caculation\n",
    "                batch_loss = loss_measurement(logits.reshape(-1, logits.size(-1)), batch_labels.reshape(-1))                  # flatten, average loss of -log(logit score)\n",
    "\n",
    "                # go backward to tune the weights\n",
    "                optimizer.zero_grad()                               # reset the gradient\n",
    "                batch_loss.backward()                                   # compute gradients and do backpropagetion to see in which directions the weights should change\n",
    "                torch.nn.utils.clip_grad_norm_(self.my_bert_model.parameters(), 1.0)    # limit the gradient values if they too large for stability\n",
    "                optimizer.step()                                                        # update the weights i the direction that reduce loss\n",
    "\n",
    "                train_total_loss += batch_loss.item()\n",
    "                train_batches_n += 1\n",
    "\n",
    "            average_train_loss = train_total_loss / train_batches_n\n",
    "\n",
    "\n",
    "            ###### validation\n",
    "            with torch.no_grad():\n",
    "                self.my_bert_model.eval()   # no weightes changes, only the exist model\n",
    "\n",
    "                validation_total_loss = 0\n",
    "                validation_batches_n = 0\n",
    "                total_words = 0\n",
    "                correct_words = 0\n",
    "\n",
    "                for index in range(0, len(validation_words), batch_size):   # for each batch\n",
    "                    batch_words = validation_words[index : index + batch_size]\n",
    "                    batch_mask_inputs, batch_labels = self.convert_batch_words_to_masked_words(batch_words, max_mask_letter_rate)\n",
    "\n",
    "                    # move the bach forward to see prediction\n",
    "                    logits = self.my_bert_model.apply_the_network(batch_mask_inputs, padding_mask = (batch_mask_inputs == 0))\n",
    "                    loss_validation = loss_measurement(logits.reshape(-1, logits.size(-1)), batch_labels.reshape(-1))\n",
    "\n",
    "                    validation_total_loss += loss_validation.item()\n",
    "                    validation_batches_n += 1\n",
    "\n",
    "                    # compute accuracy to each word (the loss is per position)\n",
    "                    predictions = logits.argmax(dim = 2)\n",
    "                    for i in range(len(batch_words)):\n",
    "                        masked_positions = (batch_labels[i] != -100)\n",
    "                        all_predicted_masked_letter_correct = (predictions[i][masked_positions] == batch_labels[i][masked_positions]).all()\n",
    "                        if all_predicted_masked_letter_correct:\n",
    "                            correct_words += 1\n",
    "                        total_words +=1\n",
    "\n",
    "\n",
    "                average_validation_loss = validation_total_loss / validation_batches_n\n",
    "                accuracy = correct_words / total_words * 100\n",
    "\n",
    "            epoch_duration_time_in_minutes = (time.time() - epoch_start_time) / 60\n",
    "\n",
    "            print(f'Finish epoch number {epoch + 1} in {epoch_duration_time_in_minutes:.2f} minutes, average_batches_train_loss: {average_train_loss:.2f}, average_batches_validation_loss: {average_validation_loss:.2f}, accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "            ###### define early stopping\n",
    "\n",
    "            if average_validation_loss < best_validation_loss:\n",
    "                best_validation_loss = average_validation_loss\n",
    "                patience_counter = 0\n",
    "                best_model_so_far = copy.deepcopy(self.my_bert_model.state_dict())      \n",
    "                torch.save(self.my_bert_model.state_dict(), 'hangman_model_12_09.pt')   # save to disk\n",
    "                print('Best model saved to disk')\n",
    "                if run_in_googel_colab == True:\n",
    "                    torch.save(self.my_bert_model.state_dict(), '/content/drive/MyDrive/hangman_model_12_09.pt')\n",
    "                    print(\"Model saved to Google Drive successfully!\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= after_what_number_of_failed_epoch_to_stop:\n",
    "                    self.my_bert_model.load_state_dict(best_model_so_far)                        \n",
    "                    print(f'Early stopping in epoch {epoch +1}, load best model.')\n",
    "                    break\n",
    "\n",
    "        return self.my_bert_model, accuracy\n",
    "\n",
    "    def hypert_parameter_search_and_full_training(self, rate_for_hyperparameters_sample_size):\n",
    "\n",
    "        # create random subset of the dataset - take 10% of the dataset\n",
    "        train_words_size = int(len(self.train_words) * rate_for_hyperparameters_sample_size)\n",
    "        validation_words_size = int(len(self.validation_words) * rate_for_hyperparameters_sample_size)\n",
    "\n",
    "        train_words_sample_for_hyper_parameter_search = random.sample(self.train_words, train_words_size)\n",
    "        validation_words_sample_for_hyper_parameter_search = random.sample(self.validation_words, validation_words_size)\n",
    "\n",
    "        # find the len of the longest word\n",
    "        max_word_length = max(len(word) for word in self.train_words + self.validation_words)\n",
    "\n",
    "        # creat a grid to parameters search\n",
    "        paramerters_to_search = {'model_dim_n': [128, 256], 'heads_n':  [4 ,8], 'layers_n': [4, 8],                                   \n",
    "                                 'max_word_length': [max_word_length], 'dropouts_rate': [0.1, 0.3], 'batch_size': [32, 64],\n",
    "                                 'learning_rate':  [0.0001, 0.0002], 'max_mask_letter_rate': [0.5, 0.7]}\n",
    "        \n",
    "        paramerters_to_search = {'model_dim_n': [256], 'heads_n':  [4], 'layers_n': [4],                                   # I saved only the parameter that perform best to try repreduce results on different dataset\n",
    "                         'max_word_length': [max_word_length], 'dropouts_rate': [0.1], 'batch_size': [32],\n",
    "                         'learning_rate':  [0.0001], 'max_mask_letter_rate': [0.5]}\n",
    "\n",
    "        # apply each option in the parmeter combination\n",
    "        parameters_and_accuracy = []\n",
    "\n",
    "        parameters_groups = list(paramerters_to_search)\n",
    "        parameters_groups_values = list(paramerters_to_search.values())\n",
    "\n",
    "        for parameters_group in itertools.product(*parameters_groups_values):\n",
    "            single_parameters_combination = dict(zip(parameters_groups, parameters_group))\n",
    "\n",
    "            if single_parameters_combination['model_dim_n'] % single_parameters_combination['heads_n'] != 0:       \n",
    "                continue\n",
    "\n",
    "            # initalize my bert model for this combination and train it\n",
    "            self.my_bert_model = MyBertModelForHangman(model_dim_n = single_parameters_combination['model_dim_n'], heads_n = single_parameters_combination['heads_n'],\n",
    "                                                       layers_n = single_parameters_combination['layers_n'], max_word_length = single_parameters_combination['max_word_length'],\n",
    "                                                       dropouts_rate = single_parameters_combination['dropouts_rate']).to(self.running_device)\n",
    "            _, combination_accuracy = self.bert_training(epochs_n = 3, train_words = train_words_sample_for_hyper_parameter_search, validation_words = validation_words_sample_for_hyper_parameter_search,\n",
    "                                                         batch_size = single_parameters_combination['batch_size'], learning_rate = single_parameters_combination['learning_rate'],\n",
    "                                                         after_what_number_of_failed_epoch_to_stop = 2, max_mask_letter_rate = single_parameters_combination['max_mask_letter_rate'])\n",
    "\n",
    "            parameters_and_accuracy.append({'parameters_combination': copy.deepcopy(single_parameters_combination), 'accuracy': combination_accuracy})\n",
    "\n",
    "            print(f' parameters_combination of: {single_parameters_combination} have accuracy of: {combination_accuracy:.2f}')\n",
    "\n",
    "        combination_with_highest_accuracy = max(parameters_and_accuracy, key = lambda x: x['accuracy'])\n",
    "        print(f\"The best combination parameters is {combination_with_highest_accuracy['parameters_combination']} with accuracy of {combination_with_highest_accuracy['accuracy']}.\")\n",
    "\n",
    "        # run the model with the best parameters on all the dataset\n",
    "        combination_with_highest_accuracy = combination_with_highest_accuracy['parameters_combination']      # stor the best parameter properly\n",
    "        self.my_bert_model = MyBertModelForHangman(model_dim_n = combination_with_highest_accuracy['model_dim_n'], heads_n = combination_with_highest_accuracy['heads_n'],\n",
    "                                                   layers_n = combination_with_highest_accuracy['layers_n'], max_word_length = combination_with_highest_accuracy['max_word_length'],\n",
    "                                                   dropouts_rate = combination_with_highest_accuracy['dropouts_rate']).to(self.running_device)\n",
    "        self.my_bert_model, accuracy = self.bert_training(epochs_n = 1000, train_words = self.train_words, validation_words = self.validation_words,\n",
    "                                                     batch_size = combination_with_highest_accuracy['batch_size'], learning_rate = combination_with_highest_accuracy['learning_rate'],\n",
    "                                                     after_what_number_of_failed_epoch_to_stop = 10, max_mask_letter_rate = combination_with_highest_accuracy['max_mask_letter_rate'])\n",
    "\n",
    "        return self.my_bert_model, accuracy\n",
    "\n",
    "\n",
    "\n",
    "class Play_Hangman(object):\n",
    "    def __init__(self):\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []\n",
    "\n",
    "        self.incorrect_guesses = 0  # this line added\n",
    "        self.train_dataset_path = 'words_250000_train.txt' # I created path to the train data:\n",
    "        self.train_and_load_my_bert_model()\n",
    "        self.full_dictionary = self.build_dictionary(self.train_dataset_path)\n",
    "\n",
    "        self.guessed_letters = []\n",
    "\n",
    "\n",
    "\n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "\n",
    "    def train_and_load_my_bert_model(self):\n",
    "        if model_allready_trained == True:\n",
    "            self.bert_instance = ApplyBert(self.train_dataset_path)\n",
    "            self.my_bert_model = MyBertModelForHangman(\n",
    "                model_dim_n=256,\n",
    "                heads_n=4,\n",
    "                layers_n=4,\n",
    "                max_word_length=29,\n",
    "                dropouts_rate=0.1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        \n",
    "            self.my_bert_model.load_state_dict(torch.load(\"hangman_model_12_09.pt\", map_location=torch.device('cpu')))\n",
    "            self.my_bert_model.eval()\n",
    "            print(f\"Model loaded successfully\")\n",
    "            \n",
    "        else:\n",
    "            self.bert_instance = ApplyBert(self.train_dataset_path)\n",
    "            self.my_bert_model, _ = self.bert_instance.hypert_parameter_search_and_full_training(rate_for_hyperparameters_sample_size = 0.02)\n",
    "            self.my_bert_model.eval()\n",
    "\n",
    "    def guess(self, masked_word):\n",
    "        \n",
    "        exists_letters = set()               # find visible letters from the masked word\n",
    "        for letter in masked_word[::2]:  \n",
    "            if letter != '_':\n",
    "                exists_letters.add(letter)\n",
    "\n",
    "        tokenized_masked_word, ind_of_masked_letter = self.my_bert_model.tokenize_mask_word(masked_word)\n",
    "        tensor_of_tokenized_masked_word = torch.tensor([tokenized_masked_word], dtype=torch.long).to(self.bert_instance.running_device)       #convert it to tensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            padding_mask = (tensor_of_tokenized_masked_word == 0)\n",
    "            logits = self.my_bert_model.apply_the_network(tensor_of_tokenized_masked_word, padding_mask=padding_mask)  # [1, max_lenth, 29]\n",
    "            scores_for_masked_position = {}\n",
    "            \n",
    "            for position in ind_of_masked_letter:\n",
    "                \n",
    "                probabilities_for_this_position = F.softmax(logits[0, position], dim = 0)    #  len 29\n",
    "                for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "                    if letter not in self.guessed_letters and letter not in exists_letters:\n",
    "                        letter_index = self.my_bert_model.dict_for_letter_to_number[letter]       # convert from letter to to token.  the letter token and the index (an integer)\n",
    "                        score = probabilities_for_this_position[letter_index].item()              # find the probability of this token\n",
    "                        scores_for_masked_position[letter] = scores_for_masked_position.get(letter, 0) + score    # add probabilities of all the masked letters\n",
    "        \n",
    "        guess_letter = max(scores_for_masked_position.items(), key = lambda x: x[1])[0]    # get the letter_with_the_highest_score\n",
    "\n",
    "        return guess_letter\n",
    "\n",
    "    def reset_game(self, word):\n",
    "        \"\"\"Reset the game state for a new word\"\"\"\n",
    "        self.guessed_letters = []\n",
    "        self.incorrect_guesses = 0  # This line was in comment but not executed\n",
    "        self.current_dictionary = [w for w in self.full_dictionary if len(w) == len(word)]\n",
    "\n",
    "    def display_word_state(self, word, guessed_letters):\n",
    "        \"\"\"Display the current state of the word with guessed letters revealed\"\"\"\n",
    "        return ' '.join([c if c in guessed_letters else '_' for c in word])\n",
    "\n",
    "\n",
    "    def play_game(self, word):\n",
    "        \"\"\"Play a complete hangman game with the given word\"\"\"\n",
    "        self.reset_game(word)\n",
    "        display_word = self.display_word_state(word, self.guessed_letters)\n",
    "        \n",
    "        print(f\"Starting game with word: {display_word}\")\n",
    "        \n",
    "        while '_' in display_word:\n",
    "            # Check if game failed due to too many incorrect guesses\n",
    "            if self.incorrect_guesses > 5:\n",
    "                print(f\"ðŸ’€ GAME FAILED! More than 5 incorrect guesses.\")\n",
    "                print(f\"The word was: {word}\")\n",
    "                return False\n",
    "                \n",
    "            guess = self.guess(display_word)\n",
    "            \n",
    "            if guess in self.guessed_letters:\n",
    "                print(f\"Already guessed letter: {guess}\")\n",
    "                continue\n",
    "                \n",
    "            self.guessed_letters.append(guess)\n",
    "            \n",
    "            # Check if guess is incorrect and increment counter\n",
    "            if guess not in word:\n",
    "                self.incorrect_guesses += 1\n",
    "                \n",
    "            display_word = self.display_word_state(word, self.guessed_letters)\n",
    "            print(f\"Guess: {guess} -> {display_word} | Incorrect: {self.incorrect_guesses}/5\")\n",
    "        \n",
    "        print(f\"SUCCESS! Word guessed: {word}\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    def test_multiple_games(self, num_games=5):\n",
    "        \"\"\"Test the algorithm on multiple random words\"\"\"\n",
    "        total_guesses = 0\n",
    "        successful_games = 0\n",
    "        failed_games = 0\n",
    "        \n",
    "        for i in range(num_games):\n",
    "            word = random.choice(self.full_dictionary)\n",
    "            print(f\"\\n=== Game {i+1}: Testing word '{word}' ===\")\n",
    "            \n",
    "            try:\n",
    "                success = self.play_game(word)\n",
    "                if success:\n",
    "                    successful_games += 1\n",
    "                    total_guesses += len(self.guessed_letters)\n",
    "                else:\n",
    "                    failed_games += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error during game: {e}\")\n",
    "                failed_games += 1\n",
    "        \n",
    "        print(f\"\\n=== FINAL RESULTS ===\")\n",
    "        print(f\"Successful games: {successful_games}/{num_games}\")\n",
    "        print(f\"Failed games: {failed_games}/{num_games}\")\n",
    "        if successful_games > 0:\n",
    "            avg_guesses = total_guesses / successful_games\n",
    "            print(f\"Average guesses per successful game: {avg_guesses:.2f}\")\n",
    "            print  (f\"Accuracy of {successful_games /  num_games} in {failed_games + successful_games} games\")\n",
    "\n",
    "# Usage examples:\n",
    "games = Play_Hangman()\n",
    "\n",
    "# Test single random word\n",
    "print(\"=== Single Random Word Test ===\")\n",
    "random_word = random.choice(games.full_dictionary)\n",
    "success = games.play_game(random_word)\n",
    "\n",
    "# Test multiple words to see win/loss rate\n",
    "print(\"\\n=== Multiple Words Test ===\")\n",
    "games.test_multiple_games(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabec8d-2967-49a1-92c8-cafdf3060a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
